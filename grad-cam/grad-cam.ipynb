{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM implementation in PyTorch\n",
    "Source: https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG19 and setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"./data/\", transform=transform)\n",
    "\n",
    "# dataloader with single image\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # get pretrained VGG19 network\n",
    "        self.vgg = vgg19(pretrained=True)\n",
    "        \n",
    "        # disect the network to access last convolutional layer\n",
    "        self.feature_conv = self.vgg.features[:36]\n",
    "        \n",
    "        # get maxpool\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # get the classifier of vgg19\n",
    "        self.classifier = self.vgg.classifier\n",
    "        \n",
    "        # place holder for gradients\n",
    "        self.gradients = None\n",
    "        \n",
    "    # hook for gradients of the activation\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_conv(x)\n",
    "        \n",
    "        # register hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        \n",
    "        # apply the remaining pooling\n",
    "        x = self.max_pool(x)\n",
    "#         x = self.vgg.avgpool(x)\n",
    "        # x = x.view((1, -1))\n",
    "        x = x.view((-1, 512 * 7 * 7))\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "    \n",
    "    def get_activations(self, x):\n",
    "        return self.feature_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG()\n",
    "\n",
    "vgg.eval() # Set to eval to avoid random results\n",
    "\n",
    "# get an image from dataset\n",
    "img, _ = next(iter(dataloader))\n",
    "\n",
    "# get prediction\n",
    "pred = vgg(img)\n",
    "labelid = pred.argmax(dim=1)\n",
    "score = pred.max(dim=1)\n",
    "\n",
    "print(\"label id = {}    score = {}\".format(labelid.numpy(), score.values.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "pred = vgg(img)\n",
    "labelid = pred.argmax(dim=1)\n",
    "\n",
    "# get the gradient of the output with respect to the output of the model\n",
    "pred[:, labelid[0]].backward()\n",
    "\n",
    "# pull the gradients out of the model\n",
    "gradients = vgg.get_activations_gradient()\n",
    "\n",
    "# pool the gradients across channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "# get the activations of the last convolutional layer\n",
    "activations = vgg.get_activations(img).detach()\n",
    "\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "    \n",
    "# average the channels of activation\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# ReLU on top of the heatmap\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"./data/elephant/elephant.jpeg\")\n",
    "heatmap = cv2.resize(heatmap.numpy(), (img.shape[1], img.shape[0]))\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "canvas = heatmap * 0.4 + img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(canvas[:,:,::-1] / 1.4 /255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze muliple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = ImageFolderWithPaths(root=\"./data/\", transform=transform)\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (img, y, path) in dataloader:\n",
    "    # get prediction\n",
    "    pred = vgg(img)\n",
    "    labelid = pred.argmax(dim=1)\n",
    "    \n",
    "    print(path, labelid)\n",
    "\n",
    "    # get the gradient of the output with respect to the output of the model\n",
    "    pred[:, labelid[0]].backward()\n",
    "\n",
    "    # pull the gradients out of the model\n",
    "    gradients = vgg.get_activations_gradient()\n",
    "\n",
    "    # pool the gradients across channels\n",
    "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "    # get the activations of the last convolutional layer\n",
    "    activations = vgg.get_activations(img).detach()\n",
    "\n",
    "    # weight the channels by corresponding gradients\n",
    "    for i in range(512):\n",
    "        activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "    # average the channels of activation\n",
    "    heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "    # ReLU on top of the heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "    # normalize the heatmap\n",
    "    heatmap /= torch.max(heatmap)\n",
    "\n",
    "    # draw the heatmap\n",
    "#     plt.matshow(heatmap.squeeze())\n",
    "    \n",
    "    img = cv2.imread(path[0])\n",
    "    heatmap = cv2.resize(heatmap.numpy(), (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    canvas = heatmap * 0.4 + img * 0.60\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.imshow(canvas[:,:,::-1] /255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-1.6.0]",
   "language": "python",
   "name": "conda-env-pytorch-1.6.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
